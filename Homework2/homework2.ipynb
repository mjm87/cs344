{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise: Spam I am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of Paul Grahamâ€™s \"A Plan for Spam\" algorithm, we'll create a small spam filter using the following corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_corpus = [[\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\"], [\"i\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However since I'm too lazy to properly go through and evaluate each \"message\" separately, I'll just combine the spam/ham messages together before evaluating the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_corpus = [\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\", \"i\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]\n",
    "ham_corpus = [\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\", \"i\", \"do\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're also going to want to have a combined token list with all the words that exist in either corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = spam_corpus + ham_corpus\n",
    "all_tokens = list(dict.fromkeys(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first step towards our little spam filter will be to count up the occurrences of each word and store them in a dictionary, for both the spam corpus and the ham corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 3, 'am': 2, 'spam': 2, 'do': 1, 'not': 1, 'like': 1, 'that': 1, 'spamiam': 1, 'green': 0, 'eggs': 0, 'and': 0, 'ham': 0}\n{'i': 2, 'am': 0, 'spam': 0, 'do': 2, 'not': 0, 'like': 1, 'that': 0, 'spamiam': 0, 'green': 1, 'eggs': 1, 'and': 1, 'ham': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_counts = {}\n",
    "ham_counts = {}\n",
    "\n",
    "# initializing every token to 0\n",
    "for token in all_tokens:\n",
    "    spam_counts[token] = 0\n",
    "    ham_counts[token] = 0\n",
    "\n",
    "# - the spam corpus\n",
    "for token in spam_corpus:\n",
    "    spam_counts[token] += 1\n",
    "\n",
    "# - the ham corpus\n",
    "for token in ham_corpus:\n",
    "    ham_counts[token] += 1\n",
    "    \n",
    "print(spam_counts)\n",
    "print(ham_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to compute the probabilities for each of the tokens in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0.5, 'am': 0.99, 'spam': 0.99, 'do': 0.3333333333333333, 'not': 0.99, 'like': 0.3333333333333333, 'that': 0.99, 'spamiam': 0.99, 'green': 0.01, 'eggs': 0.01, 'and': 0.01, 'ham': 0.01}\n"
     ]
    }
   ],
   "source": [
    "def prob(token):\n",
    "    \n",
    "    # if the token hasn't been seen, we'll assume it's innocent\n",
    "    if token not in all_tokens:\n",
    "        return 0.4\n",
    "    \n",
    "    good = 2 * ham_counts[token] # biased to reduce false positives (i.e. good emails getting lost in spam)\n",
    "    bad = spam_counts[token] # just the number of times the word was repeated in the spam corpus\n",
    "    \n",
    "    n_bad = 2 # number of spam messages (not spam words)\n",
    "    n_good = 2 # number of good messages\n",
    "    \n",
    "    # using a count threshold of 1 instead of Graham's 5 \n",
    "    # which makes sense for a smaller corpus such as ours...\n",
    "    if good + bad >= 1:\n",
    "        return max(0.01, min(0.99, min(1.0, bad / n_bad) / (min(1.0, good / n_good) + min(1.0, bad / n_bad))))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# finding the probabilities\n",
    "probabilities = {}\n",
    "for token in all_tokens:\n",
    "    probabilities[token] = prob(token)\n",
    "\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to figure out what are the most interesting words in a sample email, and Graham limited his list to the top 15 values with the probabilities furthest from 50-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the first 15 values furthest from 50%\n",
    "def get_top_interesting_values(values, limit=15):\n",
    "    # calculate the probability that the message is spam given each word separately\n",
    "    probabilities = {}\n",
    "    for i in values:\n",
    "        probabilities[i] = prob(i)\n",
    "        \n",
    "    # sort the probabilities by how \"interesting\" each value is\n",
    "    # i.e. how far away from 50% it is\n",
    "    sorted_values = sorted(probabilities.items(), key=lambda x: -abs(x[1]-0.5))\n",
    "    \n",
    "    count = 0\n",
    "    interesting = []\n",
    "\n",
    "    # grab the most \"interesting\" values to evaluate the email\n",
    "    for item in sorted_values:\n",
    "        interesting.append(item[1])\n",
    "        count += 1\n",
    "        # stop once we hit get 15 or so values\n",
    "        if count > limit:\n",
    "            break\n",
    "    \n",
    "    return interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take the most interesting words from an email and use them to evaluate if we think the email is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def is_spam(featured_probs):\n",
    "    prod = reduce((lambda x, y: x * y), featured_probs)\n",
    "    compl = reduce((lambda x, y: x * y), list(map(lambda x: 1 - x, featured_probs)))\n",
    "    value = prod / (prod + compl)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So then to go through and test everything, we can use the following helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(email):\n",
    "    values =  email.split()\n",
    "    top_15 = get_top_interesting_values(values)\n",
    "    print(\"P(spam) = \" + str(is_spam(top_15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for a couple of trivial samples, we get the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam) = 0.9998979800040808\nP(spam) = 2.576524716472776e-07\nP(spam) = 1.0410203448479832e-08\nP(spam) = 6.870729626826629e-07\n"
     ]
    }
   ],
   "source": [
    "spam_email = \"i not spam\"\n",
    "mixed_email = \"i do not like green eggs and ham\"\n",
    "ham_email = \"green eggs and ham\"\n",
    "foreign_email = \"green eggs and Sam\"\n",
    "\n",
    "evaluate(spam_email)\n",
    "evaluate(mixed_email)\n",
    "evaluate(ham_email)\n",
    "evaluate(foreign_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless there's something particularly Bayesian about his evaluation of the email (the second portion of his code-block), nothing strikes me as particularly Bayesian about his approach, barring potentially is it's adaptability to an expanding corpus. For the most part it just looks like he's using a frequentist approach, tweaking the notions to bias it a little to avoid false positives, etc. Perhaps the tweaks he's added could be considered priors? Otherwise, it's just his interest in expanding his corpus over time that feels at all Bayesian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Rain, Rain, Go Away!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the aima implementation of the Bayes Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probability import BayesNet, enumeration_ask\n",
    "\n",
    "# Utility variables\n",
    "T, F = True, False\n",
    "\n",
    "cloudy = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T: 0.1, F: 0.5}),\n",
    "    ('Rain', 'Cloudy', {T: 0.8, F: 0.2}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T): .99, (T, F): 0.90, (F, T): 0.90, (F, F): 0.0})\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a full joint probability distribution for this domain, none of the 4 variables would be considered independent of each other and so if we were to create a truth-table for it, we would have to look at the P(\"wet\"|every combo of cloudy, sprinkling, rain) which would invovle finding 16 different probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Bayesian Network, however, we can assume that since Sprinkler, Rain, and Wet Grass are clearly dependent on Cloudy, we can think of them as being conditionally independent, so there would be 3 independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Cloudy) is just given as P(C) or <0.5, 0.5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 0.9, True: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(enumeration_ask('Sprinkler',dict(Cloudy=T), cloudy).show_approx())      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Sprinkler|cloudy) is also right off the table: P(S|C) or <0.10, 0.90>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 0.9, True: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(enumeration_ask('Sprinkler',dict(Cloudy=T), cloudy).show_approx())      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Cloudy|sprinkler and not raining) = $P(+c|+s,-r) = P(+c,+s,-r) / P(+s,-r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= \\frac{P(+c)P(+s|+c)P(-r|+c)}{\\sum_c P(c)P(+s|c)P(-r|c)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04761904761904762\n0.9523809523809523\nFalse: 0.952, True: 0.0476\n"
     ]
    }
   ],
   "source": [
    "numerator = 0.5 * 0.1 * 0.2\n",
    "denominator = 0.1 * 0.5 * 0.2 + 0.5 * 0.8 * 0.5\n",
    "print(numerator / denominator)\n",
    "print(1 - numerator / denominator)\n",
    "print(enumeration_ask('Cloudy',dict(Sprinkler=T,Rain=F), cloudy).show_approx()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(WetGrass | cloudy, sprinkler, raining) = $\\alpha\\sum P(+w | s, r) * P(s|c) * P(r|c) * P(C) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36900000000000005"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val1 = 0.99 * ( 0.1 * 0.8 * 0.5 + 0.5 * 0.2 * 0.5) \n",
    "+ 0.90 * ( 0.1 * 0.2 * 0.5 + 0.5 * 0.8 * 0.5) \n",
    "+ 0.90 * ( 0.9 * 0.8 * 0.5 + 0.5 * 0.2 * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04100000000000001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val2 = 0.01 * ( 0.1 * 0.8 * 0.5 + 0.5 * 0.2 * 0.5) \n",
    "+ 0.10 * ( 0.1 * 0.2 * 0.5 + 0.5 * 0.8 * 0.5) \n",
    "+ 0.10 * ( 0.9 * 0.8 * 0.5 + 0.5 * 0.2 * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 / (val1 + val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_wet = val1 * alpha\n",
    "prob_wet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009999999999999998"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dry = val2 * alpha\n",
    "prob_dry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 0.01, True: 0.99\n"
     ]
    }
   ],
   "source": [
    "print(enumeration_ask('WetGrass',dict(Sprinkler=T,Rain=T,Cloudy=T), cloudy).show_approx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Cloudy|grass is not wet)= $$ P(+c|-w) = P(+c,-w) / P(-w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\frac{\\sum_s \\sum_r P(+c,-w,s,r)}{\\sum_s \\sum_r \\sum_c P(c, -w, s, r)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\frac{\\sum_s \\sum_r P(-w|s,r)P(s|+c)P(r|+c)P(+c)}{\\sum_s \\sum_r \\sum_c P(-w|s,r)P(s|c)P(r|c)P(c)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5945945945945945"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator = (0.01) * (0.1 * 0.8 * 0.5) + (0.1) * (0.9 * 0.8 * 0.5) + (0.1) * (0.1 * 0.2 * 0.5)\n",
    "denominator = (0.01) * (0.1 * 0.8 * 0.5 + 0.5 * 0.2 * 0.5) + (0.10) * (0.9 * 0.8 * 0.5 + 0.5 * 0.2 * 0.5) + (0.10) * (0.1 * 0.2 * 0.5 + 0.5 * 0.8 * 0.5)\n",
    "numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = (0.01) * (0.1 * 0.8 * 0.5) + (0.1) * (0.9 * 0.8 * 0.5) + (0.1) * (0.1 * 0.2 * 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 0.639, True: 0.361\n"
     ]
    }
   ],
   "source": [
    "print(enumeration_ask('Cloudy', dict(WetGrass=F), cloudy).show_approx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I am aware that my theoretical answer and my algorithmic answer don't match, I'm not certain I can figure out the reason. If I understood the Udacity Probalistic Inference video on Enumeration, I'm pretty sure I'm approaching the problem correctly, but given that my answers don't match, I suppose I'm still missing something conceptually. That or I made an arithmetic mistake..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
