{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzsAhFx8sNEE",
    "colab_type": "text"
   },
   "source": [
    "##1. Essay: Are Neural Nets Destined to Fail\n",
    "  \n",
    "In the light of the downfalls of the once popular perceptrons and expert systems, it definitely makes sense to keep a tempered view of the current hype around deep learning and neural networks. These new techniques are making remarkable strides forward, allowing AI to solve problems that were too challenging for the perceptrons and expert systems.\n",
    "\n",
    "\n",
    "Surrounded by all the hype, it's challenging to discern any particular weaknesses with neural nets akin to the insurmountable brittleness of the expert systems. However, if I had to guess at which problems are likely to be most exacerpated as neural nets grow: I'm guessing both the obscurity of our black box approach will be a contender. With increasing concern around the validity of some of these algorithms and their systemic impact in our lives (see *Weapons of Math Destruction* and the like), being able to peak inside the \"mind of the AI\" will prove invaluable, and might not be possible with our current approach. Another drawback might come from the data collection side. Perhaps there might be problems that will prove too challenging to find sufficient data for or the data's biases will prove insurmountable. Finding data to create a \"wise\" AI seems unlikely, since we can hardly figure out how to become wiser ourselves.\n",
    "\n",
    "On the other hand, the capabilities of deep learning seem to be particularly well-suited for a variety of tasks. With more big data and more GPUs to process the data with, neural networks have done quite well. I suspect, while we might find better ways to enhance or augment these approaches, deep learning is likely to be around for the long-haul in some capacity or another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5st2SaNNDsL_",
    "colab_type": "text"
   },
   "source": [
    "##2. Backpropagation Computation Example: XOR: [1,1] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPsZpiOioXic",
    "colab_type": "text"
   },
   "source": [
    "1. Fill in random weights (same as in class).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    &\\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\\n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} \\\\\n",
    "    &\\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1} \n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "    \n",
    "2. Compute the output for one sample (XOR: `[1, 1]` &rarr; `0`).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    o_j &= \n",
    "    \\begin{bmatrix}\n",
    "    1 & 1 \\\\ \n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    1 * 0.11 + 1 * 0.21 & 1 * 0.12 + 1 * 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.34 & 0.20\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.34 * 0.14 + 0.20 * 0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &= 0.0748\n",
    "    \\end{aligned}\n",
    "    \\\\\n",
    "    $\n",
    "\n",
    "3. Compute the error (and more importantly, the delta).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    L_2Error &= (1 - 0.0748)^2 \\\\\n",
    "    &= 0.8559 \\\\\n",
    "    \\Delta_{o_1} &= (1 - 0.0748) \\\\\n",
    "    &= 0.9252 \\\\\n",
    "    \\end{aligned}$\n",
    "\n",
    "4. Backpropagate updates back through the network, assuming: \n",
    "    $learning\\_rate = 0.05$; \n",
    "    identity activation functions for all nodes(f(x)=x and f'(x) = 1).\n",
    "     \n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + 0.05 \\cdot \n",
    "    \\begin{bmatrix}\n",
    "    0.34 \\\\ \n",
    "    0.20 \n",
    "    \\end{bmatrix} \\cdot 1.0 \\cdot 0.9252 \\\\\\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 0.32 * 1.0 * 0.9252 \\\\\n",
    "    0.05 * 0.20 * 1.0 * 0.9252 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} +\n",
    "    \\begin{bmatrix}\n",
    "    0.0148032 \\\\\n",
    "    0.009252 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0.1548032 \\\\ \n",
    "    0.159252\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + 0.05 \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    1 & 1 \\\\ \n",
    "    1 & 1\n",
    "    \\end{bmatrix} \\cdot 1.0 \\odot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 & 0.15 \\\\ \n",
    "    0.14 & 0.15\n",
    "    \\end{bmatrix} \\cdot 0.9252 \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
    "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    0.14 * 0.9252 & 0.15 * 0.9252\\\\ \n",
    "    0.14 * 0.9252 & 0.15 * 0.9252 \n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 & 0.05 \\\\ \n",
    "    0.05 & 0.05 \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    0.1342 & 0.1438 \\\\\n",
    "    0.1342 & 0.1438\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 0.1342 & 0.05 * 0.1438 \\\\\n",
    "    0.05 * 0.1342 & 0.05 * 0.1438\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} +     \n",
    "    \\begin{bmatrix}\n",
    "    0.0067 & 0.0072 \\\\\n",
    "    0.0067 & 0.0072\n",
    "    \\end{bmatrix} \\\\ &= \n",
    "    \\begin{bmatrix}\n",
    "    0.1167 & 0.1272 \\\\\n",
    "    0.2167 & 0.0872\n",
    "    \\end{bmatrix}  \n",
    "    \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBCQgt0NDzZX",
    "colab_type": "text"
   },
   "source": [
    "## 3. Keras-based CNN\n",
    "\n",
    "Herein begins the most fashionable CNN we've done yet!\n",
    "\n",
    "First we just need to pull in the images from *fashion_mnist*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YV2aMS2fD7EW",
    "colab_type": "code",
    "outputId": "dcc676e3-88fb-4252-ccee-5366a55d0d46",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 3us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgnn6RErjUdT",
    "colab_type": "text"
   },
   "source": [
    "Then we can create a model with convolution layers. The one below is the same structure as the example with the regular *mnist* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SWgjmRsfEmus",
    "colab_type": "code",
    "outputId": "184a6ee7-aa5b-4b5a-fd95-ceaa2754676c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdY0SbMfjmBG",
    "colab_type": "text"
   },
   "source": [
    "And then we can train the model and see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "L3aCuLPHEpKg",
    "colab_type": "code",
    "outputId": "2ffd0980-c9b4-4d65-eded-6ab0c5c6a194",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 0.5399 - acc: 0.8002\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.3337 - acc: 0.8785\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.2838 - acc: 0.8952\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.2528 - acc: 0.9064\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.2297 - acc: 0.9150\n",
      "10000/10000 [==============================] - 4s 362us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27244248328208925, 0.9053]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnERMYsTjwSr",
    "colab_type": "text"
   },
   "source": [
    "Which seems to do a decent job. Not unsurprisingly, it does a little worse with the test data than it does with the training data, but a drop from 0.915 to 0.905 is pretty tolerable.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Homework4",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
