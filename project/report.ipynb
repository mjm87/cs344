{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlPy63Fb-BrA",
        "colab_type": "text"
      },
      "source": [
        "## Vision\n",
        "\n",
        "Hoping to encorporate some of the advancements in artificial intelligence into games made even by indie developers, Unity Technologies developed *Puppo, The Corgi* to demonstrate the capabilities of it's ML-Agents toolkit. And in a world where state-machines are state-of-the-art \"AI\", even simple tools to leverage machine learning and neural network techniques is most appreciated: opening up the doors not necessarily to smarter non-playable characters, but perhaps towards automated beta testing by autonomous agents or the use of generative adversarial networks to create artwork to populate the worlds, it feels like there are many useful applications of artificial intelligence technologies to aid game-developers in creating the worlds we love to explore. Hopefully then, the ML-Agents toolkit and perhaps even the *Puppo* example can serve as valuable starting points for developers to dip their toes into the world of AI.\n",
        "\n",
        "My project's starting point, *Puppo, The Corgi* is a sample project including the cute but helpless titular canine who starts off with neither animations nor any form of movement rather than the slight alterations to the various cylindrical limbs. But through training, a model is developed that allows him to move his little legs in ways that at first propel him towards his stick, and later to create a cute if perhaps comical custom animation for the puppy.\n",
        "\n",
        "This project seeks to build on the foundations of  *Puppo, The Corgi* example project and to create a reinforcement learning model that is able to pilot a small spaceship through a very small labyrinth. As opposed to *Puppo* this poses a couple of additional challenges. Whereas Puppo's legs were confined to allowable motions of a joint, the rockets on the space-ship would be free for the model to mess with as much as it wants. Additionally, while Puppo's legs did react to physics, the forces applied to them were affected by gravity, collisions and drag, however with the spaceship, a force once applied will remain applied until it's countered by an equal and opposite force. With the layout of the rocket thrusters on the spaceship I'm using, this means that even to go straight will often require the model to balance out it's forces so that it doesn't go careening off into a wall. This allows us the opportunity to explore a problem in which preceding decisions have to be taken into account for the next decision. Finally, the labyrinth itself gives us the opportunity to potentially explore additional problems such as collision avoidance and even some pathfinding. Hopefully these extensions will allow us to explore the capabilities of the ML-Agents a little bit beyond the crowd-pleasing *Puppo* and get an understanding of some of the out-of-the-box tools that Unity Technologies is opening up to hobbyist game developers in addition to AI researchers who are looking to create sandboxes to test out new machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nft8qqa-B5f",
        "colab_type": "text"
      },
      "source": [
        "## Background\n",
        "\n",
        "The technologies I'm working with in this project are all pulled from Unity's ML-Agents toolkit which is a relatively new set of tools provided by Unity to bring machine learning techniques into the world of game development. At the end of the day, the toolkit is just a convenient wrapper that allows developers to connect via a socket to an external Tensorflow script and is composed of a suite of tools to make that more palateable to gameplay programmers and artists. \n",
        "\n",
        "**Agent:**\n",
        "This could be a slime enemy, a cute corgi, a spaceship or whatever the developer wants to accept the model's output and transform it into in-game actions. An agent will be connected to a brain.\n",
        "\n",
        "**Brain:**\n",
        "The brain is the home for the machine learning model. While training, the brain will serve as the conduit between Unity3D and the external Tensorflow script (via the academy), whereas in production it will serve as the repository for the compiled byte equivalent of the model. A brain can be either external (connected to the Tensorflow script), internal (using the compiled byte model), heuristic (not using the model at all), or driven by player input. Several agents can be associated with a single brain or model.\n",
        "\n",
        "**Academy:**\n",
        "The academy is used as a way to house multiple brains together as well as serving as the final conduit between the brains and the external Tensorflow script.\n",
        "\n",
        "![See the Learning Environment Diagram here: https://www.github.com/Unity-Technologies/ml-agents/docs/images/learning_environment.png ](https://www.github.com/Unity-Technologies/ml-agents/docs/images/learning_environment.png)\n",
        "\n",
        "There are a couple approaches they've provided out-of-the-box: basic reinforcement learning, as well as some variations including a curriculum and imitation approaches in additional to smaller tweaks like the inclusion of an LSTM network in addition to providing a python API for you to interface with the Tensorflow model directly from within your Unity3d project. \n",
        "The [curriculum](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Curriculum-Learning.md) approach allows you to gradually scale the difficulties of the scenarios the model is being trained on, much in the same way that game developers are used to increasing the learning curve for new players in say a puzzle game. \n",
        "The [imitation](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Imitation-Learning.md) approach, similarly, is also a bit more intuitive for game developers since the model learns from observing a player in say a racing game. Both of these approaches, curriculum learning and imitation, intrigue me but were unfortunately outside the scope of what I would be able to figure out for this project.\n",
        "\n",
        "The reinforcement learning algorithm implemented by default is the *Proximal Policy Optimization* (PPO) algorithm. It has apparently become the default reinforcement learning algorithm at OpenAI thanks to it's simplicity, ease of use, and general performance ([see here](https://openai.com/blog/openai-baselines-ppo/)). It's essentially a constrained policy gradient method designed to speed up the learning process similar to alternative approaches such as TRPO and ACER. \n",
        "\n",
        "To get a reinforcement model up and running in Unity,  involves considering and defining the following three components:\n",
        "\n",
        "**Observations:**\n",
        "These are typically numeric values that combine to form the relevant state of the world as our agent knows it. These can be things like the agent's velocity, the agent's distance to the bone, the agent's hunger level, or whatever. It can also include visual input from a camera in the scene to depict what the agent sees, a top-down view of the map, or whatever else the developer thinks the agent (and the model) should factor in.\n",
        "\n",
        "**Actions:**\n",
        "These are the options available to the agent through which to modify the game-state. These are typically driven by the model's outputted float values. \n",
        "\n",
        "**Rewards:**\n",
        "Rewards are given to the agent upon doing something well and serve as the model's main avenue into discovering the best course of action. Negative rewards or punishments can also be used. But rewards, postive or negative, are the developer's best tools for outlining the path it hopes the AI will follow. Rewards can be for end-goal situations like successfully fetching the bone, but can also be used to nudge the model in a certain direction such as giving a small reward for moving towards the bone or a small penalty for every timestep to spur the model to take a less circuitous route. Best practice appears to be to use honey rather than vinegar, since a harsh penalty might dissuade a model from some creative solutions to greater rewards further down the road. \n",
        "\n",
        "For more information see the toolkit's [overview](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/ML-Agents-Overview.md) or the [documentation](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Readme.md) or the mess around with the sample *Puppo* project [here](https://blogs.unity3d.com/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/) .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtDbp_cR-CEW",
        "colab_type": "text"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "*Summarize your implementation (how it extends referenced work)*\n",
        "\n",
        "Instead of working with a cute Corgi puppi, alas, I decided to take the learnings I scavenged from the sample project and create a new scene in which the model is trained to fly a space ship through a very simple labyrinth. The 3D model that I'm using was found on the Unity3d Asset store for free [here](https://assetstore.unity.com/packages/3d/vehicles/space/spaceship-by-pixel-make-99120).\n",
        "\n",
        "Beyond replacing the adorable pup with a comparatively lackluster spacecraft, additional work was necessary to make sure that the space-ship had rockets assigned in somewhat plausible locations, crashing into walls could be detected, and the labyrinth had to be constructed.\n",
        "\n",
        "![Spaceship Diagram here: https://github.com/mjm87/cs344/project/Images/spaceship.PNG ](https://github.com/mjm87/cs344/project/Images/spaceship.PNG)\n",
        "\n",
        "The spaceship has 5 rocket thrusters that have been setup. One at the back to provide general propulsion. The other four are positioned near the circly elements on the bottoms of the wings and were intended to allow the spaceship to rotate through tight spaces and around curves. Each position serves as a point to apply both force and torque to the spaceship, providing it theoretically full movement capabilities. However as the model struggled to figure out, fairly natural forward movement and turning proved problematic to figure out with only these 5 pressure points to work with.\n",
        "\n",
        "**The Actions**\n",
        "\n",
        "The actions for the Pilot Agent that I created are fairly straightforward, the model has full control over the 5 rocket thrusters and that's it.\n",
        "Note however, that while in real-life (or at least as real-life as sci-fi space-craft get) a rocket thruster should only be able to push in one direction,\n",
        "I didn't realize that until too late in the process, so the ship has the extra special capability of using the same rocket to move both forwards and backwards. This is all set up in the PilotAgent.cs script.\n",
        "\n",
        "The vectorAction array is the list of the model's outputs which are used to determine both the power and the directionality of the forces applied at each of the rocket thruster locations. The textAction string is just a label that could be used to describe discrete actions and is disregarded here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIm0jO0g5Ok9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\tpublic override void AgentAction(float[] vectorAction, string textAction)\n",
        "    {\n",
        "        // apply what the model thinks is appropriate force at each of the rocket thruster positions\n",
        "        rb.AddForceAtPosition(transform.forward * vectorAction[0], rearRocket.position);\n",
        "        rb.AddForceAtPosition(transform.forward * vectorAction[1], rightForwardRocket.position);\n",
        "        rb.AddForceAtPosition(transform.forward * vectorAction[2], rightRearRocket.position);\n",
        "        rb.AddForceAtPosition(transform.forward * vectorAction[3], leftForwardRocket.position);\n",
        "        rb.AddForceAtPosition(transform.forward * vectorAction[4], leftRearRocket.position);\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZwQJKLDmsMe",
        "colab_type": "text"
      },
      "source": [
        "**The Observations**\n",
        "\n",
        "The observations that I was interested in grew as I tweaked with the model more and more.\n",
        "They eventually came to include principally the agent's location in the world, it's rotation, it's previous location and rotation, the distance to the nearest wall, and whether or not there were any obstacles (walls) within a small radius."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vsxji4fnQPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    public override void CollectObservations() {\n",
        "\n",
        "        // current location\n",
        "        AddVectorObs(transform.position);\n",
        "        AddVectorObs(transform.localPosition);\n",
        "\n",
        "        // current rotation\n",
        "        AddVectorObs(transform.rotation);\n",
        "        AddVectorObs(transform.localRotation);\n",
        "\n",
        "        // previous location / rotation\n",
        "        AddVectorObs(lastPosition);\n",
        "        AddVectorObs(lastRotation);\n",
        "\n",
        "        // update previous location / rotation\n",
        "        lastPosition = transform.position;\n",
        "        lastRotation = transform.rotation;\n",
        "\n",
        "        // distance to nearest wall\n",
        "        AddVectorObs(DistanceToNearestWall());\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT6NPnphn3Y3",
        "colab_type": "text"
      },
      "source": [
        "Eventually I also added the following segment of code to check whether or not there were any walls or other obstacles within a small radius of the ship. If there were, the agent would incur a slight penalty, but it seemed to make sense to add that to the observed information as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zqbT74noOPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "void RewardProximityPenalty()\n",
        "    {\n",
        "        // find all nearby obstacles\n",
        "        var obstacles = Physics.OverlapSphere(\n",
        "            position: transform.position, \n",
        "            radius: 8f\n",
        "        ).Where(x => x.tag == \"Obstacle\").ToList();\n",
        "\n",
        "        // we only really care if we're near at least one\n",
        "        bool inCloseProximity = obstacles.Count > 0;\n",
        "\n",
        "        // Inform the ship about the number of nearby objects\n",
        "        // since it's deserves to know what it's being punished for...\n",
        "        AddVectorObs(inCloseProximity);\n",
        "\n",
        "        // Penalize ship for being too close to the walls\n",
        "        if (inCloseProximity) AddReward(-0.001f);\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ao2TyCoS-q",
        "colab_type": "text"
      },
      "source": [
        "In addition to the hard-coded specifications, the agent was also provided with an 84x84 pixelized view of the scene theoretically providing the agent esentially with a layout of the labyrinth so that it could get better at not crashing into walls. This unfortunately the model never seemed to pick up the relevant features from the camera feed and continued to run into the walls anyways.\n",
        "\n",
        "![See sample image at https://github.com/mjm87/cs344/blob/master/project/Images/pixel-view.PNG](https://github.com/mjm87/cs344/blob/master/project/Images/pixel-view.PNG)\n",
        "\n",
        "It's also worth noticing that I set the Vector Stack size to 10 for the observations, which means that the observations for the previous 10 decisions will be maintained so that the 11th decision can use a historical snapshot to have some idea of what's recently occurred. Ideally this would help the rocket stabilize a bit better so that if one side of the wing was being activated previously, it might know to balance out the other side one or two steps down the road. In some ways that might just be wishful thinking, but it did appear to help out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZnyQstcqJi5",
        "colab_type": "text"
      },
      "source": [
        "**The Rewards**\n",
        "\n",
        "This is perhaps the trickiest part in this whole reinforcement learning business is the selection and refinement of the rewards given to the agent for doing this or that.\n",
        "\n",
        "As it stands at the moment, the model is \n",
        "- rewarded for moving forward\n",
        "- penalized for moving backwards\n",
        "- penalized for hitting a wall\n",
        "- penalized for getting too close to a wall\n",
        "\n",
        "\n",
        "reward_per_step = + 0.001 * moving_forward - 0.001 * moving_backward - 0.001 * being_near_a_wall - 1 * hitting_a_wall\n",
        "\n",
        "This is ultimately I suspect a far too penalizing model and a better reward model should be more positively luring the model in the right direction.\n",
        "However, ultimately it was much easier for me to programmatically define the the agent shouldn't do and my conceptions for what it should do were too nebulous to nail down. Unfortunately, \"I know it when I see it\" doesn't transcribe into code as nicely as I'd like.\n",
        "\n",
        "**The Hyperparameters**\n",
        "\n",
        "While I spent more time tweaking the observations and the rewards, the hyperparameters that I used are defined in the trainer_config.yaml file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMKjHJ9DtW40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PilotBrain:\n",
        "    normalize: true \n",
        "    num_epoch: 3 \n",
        "    time_horizon: 1000 \n",
        "    batch_size: 2048 \n",
        "    buffer_size: 20480 \n",
        "    gamma: 0.995 \n",
        "    max_steps: 1e6 \n",
        "    summary_freq: 3000 \n",
        "    num_layers: 3 \n",
        "    hidden_units: 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqrfkAJN5V7C",
        "colab_type": "text"
      },
      "source": [
        "There are a variety of other hyper-parameters that could also be added into the model. While I didn't play with them here, a recurrent LSTM network could have been added in using the recurrent parameters, or I could have played with some of the curiousity related parameters that seem to drive the model to more actively seek out rewards in a more sparsely reward-laden environment. The RNN/LSTM options were particularly appealing, but apparently don't play as well in a continuous action environment so I didn't mess with them much.\n",
        "\n",
        "**The Labyrinth**\n",
        "\n",
        "While I initially attempted to mimic *Puppo's* more free-form bone-hunting with the spaceship, I struggled to guide the model away from spinning uncontrollably away from the target. Some of my earlier issues were resultant from residual force buildup that never properly got cleared away, but ultimately proved unworkable with any of the reward functions I was attempting. To alleviate that, I turned towards a more constrained environment in which running into a wall yielded in a short and sudden stop alongside a spike of penalty points.\n",
        "\n",
        "While I had initially dreamt up a simple procedurally generated labyrinth to test the pilot agent in, this very non-labyrinthine maze with more or less a single obstacle has proved a significant challenge for my little spaceship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo62qt-m-CLv",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "*Results of your system, compare to other work*\n",
        "\n",
        "As the Tensorflow script is training, I get blessed with the following sorts of output.\n",
        "It is interesting to note how the mean reward does actually improve over 699,000 steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwScqqu51DKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 3000. Mean Reward: -2.171. Std of Reward: 1.099. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 6000. Mean Reward: -1.894. Std of Reward: 2.499. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 9000. Mean Reward: -1.773. Std of Reward: 1.564. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 12000. Mean Reward: -2.022. Std of Reward: 0.635. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 15000. Mean Reward: -1.094. Std of Reward: 1.405. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 18000. Mean Reward: -1.866. Std of Reward: 1.174. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 21000. Mean Reward: -1.474. Std of Reward: 1.301. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 24000. Mean Reward: 0.605. Std of Reward: 0.376. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 27000. Mean Reward: 0.129. Std of Reward: 0.850. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 30000. Mean Reward: 0.640. Std of Reward: 1.120. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 33000. Mean Reward: -0.024. Std of Reward: 0.559. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 36000. Mean Reward: 0.112. Std of Reward: 0.514. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 39000. Mean Reward: -0.017. Std of Reward: 2.190. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 42000. Mean Reward: -0.373. Std of Reward: 0.264. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 45000. Mean Reward: -0.278. Std of Reward: 0.649. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 48000. Mean Reward: 0.210. Std of Reward: 1.240. Training.\n",
        "              \n",
        "              After a lot more training\n",
        "\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 651000. Mean Reward: 1.642. Std of Reward: 0.703. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 654000. Mean Reward: 1.426. Std of Reward: 0.486. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 657000. Mean Reward: 1.977. Std of Reward: 0.884. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 660000. Mean Reward: 2.235. Std of Reward: 0.073. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 663000. Mean Reward: 1.601. Std of Reward: 0.366. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 666000. Mean Reward: 1.503. Std of Reward: 0.731. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 669000. Mean Reward: 0.809. Std of Reward: 0.471. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 672000. Mean Reward: 0.409. Std of Reward: 0.831. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 675000. Mean Reward: 0.601. Std of Reward: 0.358. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 678000. Mean Reward: 0.707. Std of Reward: 0.506. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 681000. Mean Reward: 1.542. Std of Reward: 0.625. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 684000. Mean Reward: 1.745. Std of Reward: 0.406. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 687000. Mean Reward: 1.696. Std of Reward: 0.810. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 690000. Mean Reward: 0.418. Std of Reward: 0.817. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 693000. Mean Reward: 0.624. Std of Reward: 0.464. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 696000. Mean Reward: 0.913. Std of Reward: 0.403. Training.\n",
        "INFO:mlagents.trainers: bumper01-0: PilotBrain: Step: 699000. Mean Reward: 0.931. Std of Reward: 0.360. Training."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJx5GPOE5RvB",
        "colab_type": "text"
      },
      "source": [
        "**Summary Graphs**\n",
        "\n",
        "![Look at reward over time here: https://github.com/mjm87/cs344/project/Images/rewardGraph.PNG](https://github.com/mjm87/cs344/project/Images/rewardGraph.PNG)\n",
        "\n",
        "It's interesting to see that over the period of training, the reward fairly significantly from run to run, but steadily rises upwards with peaks above 2 units of reward (which is only given out in 0.001 increments) is somewhat impressive. It's a little less impressive that it took nearly 600k steps to get there...\n",
        "\n",
        "![Look at the length of each run here: https://github.com/mjm87/cs344/project/Images/timeToDie.PNG](https://github.com/mjm87/cs344/project/Images/timeToDie.PNG)\n",
        "\n",
        "Instead of looking at reward, this chart looks at the episode length (or how long the space ship made it before crashing into a wall).\n",
        "This too has quite a bit of noise, but still seems to be steadily rising. But even at it's highest point, the ship apparently would around every 1000 steps. This is handy to understand the summary data above which is printed every 3000 steps which is thus 3-4 typical crash runs.\n",
        "\n",
        "**Videos**\n",
        "\n",
        "Finally to round out the before and after comparison, here's a video from a sample run towards the beginning (step 3000 or so) and from near the end (step 699000 or so).\n",
        "\n",
        "*before:*\n",
        "[step 3000ish video](https://github.com/mjm87/cs344/blob/master/project/Videos/EarlyDays.mp4)\n",
        "\n",
        "*after:*\n",
        "[step 699,000ish video](https://github.com/mjm87/cs344/blob/master/project/Videos/pretty_old.mp4)\n",
        "\n",
        "To watch them, you'll probably have to download them from github and play them in a video editor of your choice.\n",
        "Unfortunately, I didn't realize the screencapture software would record audio as well so there's some extra noise and unexpected background music that I haven't had time to edit out... Feel free to just mute it for the couple of seconds they last.\n",
        "\n",
        "**Concluding thoughts**\n",
        "\n",
        "Ultimately, it turns out that reinforcement learning even in an easy ready-made solution like ML-Agents toolkit is going to be quite involved.\n",
        "Coming up with sensible and robust reward / observation patterns seems to be key to getting the agent to detect an optimum pattern, but that seems less than ideal for solutions where we want the machine to find patterns and processes that we can't ourselves. Additionally, just like in statistical models and good old fashioned supervised learning, there are a ton of hyper-parameters to tweak as well once you're satisfied on an algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXlQ6x-f--_8",
        "colab_type": "text"
      },
      "source": [
        "##Social Implications\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-kiIT9R_BGV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Advances in Reinforcement Learning seem to be driving many of the cool technologies at large at the moment. Like with any technological advancement, there's the threat of introducing new technologies that carry along extra baggage that disrupt the environments they're placed in in unexpected ways. One of the largest concerns is often the lost jobs as new algorithms learn how to do things cheaper and faster than humans. \n",
        "\n",
        "Particularly looking at the game's industry, these sorts of things feel like toy examples: mostly designed to spark developer interest in tinkering with AI. But just as truck drivers might be rightfully afraid for their jobs with advancements in self-driving vehicles, it might not be too long before a little bit more complex examples of Puppo where reinforcement learning could automate animator's jobs away. However, especially in the game's industry, I suspect that while AI techniques will hopefully become more prevalent they will be more intending to supplement and enhance developers abilities rather than outright replacing them. And perhaps it's foolish pride or foolish faith, but I find it hard to imagine any algorithm that can automate away the creativity and ingenuity that human developers rely on to create compelling worlds which tug perhaps even at the soul of the players. And if the amount of work that I put in to get a drunken ship that can barely go in a straight line tells me anything, it'll be quite a while before anything like that is plug-n-play."
      ]
    }
  ]
}